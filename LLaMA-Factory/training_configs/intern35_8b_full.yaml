### model
model_name_or_path: ../checkpoints/base_models/InternVL3_5-8B-Pretrained-HF
image_max_pixels: 518400
# video_max_pixels: 16384
trust_remote_code: true

### method
stage: sft
do_train: true
finetuning_type: full
freeze_vision_tower: true
freeze_multi_modal_projector: false
freeze_language_model: false
# lora_rank: 16
# lora_target: all

### dataset
dataset: xxx
template: intern_vl
#template: default
cutoff_len: 4096
overwrite_cache: true
preprocessing_num_workers: 16
dataloader_num_workers: 4

### output
output_dir: xxx
logging_steps: 10
save_steps: 0.25
plot_loss: true
overwrite_output_dir: true
save_only_model: true
report_to: wandb

### train
per_device_train_batch_size: 8 # total batch size = per_device_train_batch_size * gradient_accumulation_steps * num_gpus = 128
gradient_accumulation_steps: 4
learning_rate: 2.0e-05  # In Cambrian they give this euristics: optimal lr = 4e-5 * sqrt(bs / 512)
num_train_epochs: 1.0
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
ddp_timeout: 180000000
resume_from_checkpoint: null

### eval
eval_dataset: xxx
per_device_eval_batch_size: 1
eval_strategy: steps
eval_steps: 0.125
